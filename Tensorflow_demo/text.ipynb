{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DweYe9FcbMK_"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:36.314998Z",
     "iopub.status.busy": "2020-09-22T22:26:36.314295Z",
     "iopub.status.idle": "2020-09-22T22:26:36.316277Z",
     "shell.execute_reply": "2020-09-22T22:26:36.316690Z"
    },
    "id": "AVV2e0XKbJeX"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUtoed20cRJJ"
   },
   "source": [
    "# 使用 tf.data 加载文本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ap_W4aQcgNT"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://tensorflow.google.cn/tutorials/load_data/text\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\" />在 TensorFlow.org 上查看</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/load_data/text.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\" />在 Google Colab 上运行</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/load_data/text.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\" />查看 GitHub 上的资源</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/tutorials/load_data/text.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\" />下载 notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEe3i16tQPjo"
   },
   "source": [
    "Note: 我们的 TensorFlow 社区翻译了这些文档。因为社区翻译是尽力而为， 所以无法保证它们是最准确的，并且反映了最新的\n",
    "[官方英文文档](https://tensorflow.google.cn/?hl=en)。如果您有改进此翻译的建议， 请提交 pull request 到\n",
    "[tensorflow/docs](https://github.com/tensorflow/docs) GitHub 仓库。要志愿地撰写或者审核译文，请加入\n",
    "[docs-zh-cn@tensorflow.org Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-zh-cn)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWeQAo0Ec_BL"
   },
   "source": [
    "本教程为你提供了一个如何使用 `tf.data.TextLineDataset` 来加载文本文件的示例。`TextLineDataset` 通常被用来以文本文件构建数据集（原文件中的一行为一个样本) 。这适用于大多数的基于行的文本数据（例如，诗歌或错误日志) 。下面我们将使用相同作品（荷马的伊利亚特）三个不同版本的英文翻译，然后训练一个模型来通过单行文本确定译者。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgZ9gjmPfSnK"
   },
   "source": [
    "## 环境搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:36.321329Z",
     "iopub.status.busy": "2020-09-22T22:26:36.320676Z",
     "iopub.status.idle": "2020-09-22T22:26:42.910788Z",
     "shell.execute_reply": "2020-09-22T22:26:42.910169Z"
    },
    "id": "baYFZMW_bJHh"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWVWjyIkffau"
   },
   "source": [
    "三个版本的翻译分别来自于:\n",
    "\n",
    " - [William Cowper](https://en.wikipedia.org/wiki/William_Cowper) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt)\n",
    "\n",
    " - [Edward, Earl of Derby](https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt)\n",
    "\n",
    "- [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt)\n",
    "\n",
    "本教程中使用的文本文件已经进行过一些典型的预处理，主要包括删除了文档页眉和页脚，行号，章节标题。请下载这些已经被局部改动过的文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:42.917689Z",
     "iopub.status.busy": "2020-09-22T22:26:42.916998Z",
     "iopub.status.idle": "2020-09-22T22:26:43.280114Z",
     "shell.execute_reply": "2020-09-22T22:26:43.280663Z"
    },
    "id": "4YlKQthEYlFw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n",
      "819200/815980 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n",
      "811008/809730 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n",
      "811008/807992 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/kbuilder/.keras/datasets'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "  \n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3sDy6nuXoNp"
   },
   "source": [
    "## 将文本加载到数据集中\n",
    "\n",
    "迭代整个文件，将整个文件加载到自己的数据集中。\n",
    "\n",
    "每个样本都需要单独标记，所以请使用 `tf.data.Dataset.map` 来为每个样本设定标签。这将迭代数据集中的每一个样本并且返回（ `example, label` ）对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:44.829621Z",
     "iopub.status.busy": "2020-09-22T22:26:44.162254Z",
     "iopub.status.idle": "2020-09-22T22:26:44.930984Z",
     "shell.execute_reply": "2020-09-22T22:26:44.930395Z"
    },
    "id": "K0BjCOpOh7Ch"
   },
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "  return example, tf.cast(index, tf.int64)  \n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
    "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "  labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8PHK5J_cXE5"
   },
   "source": [
    "将这些标记的数据集合并到一个数据集中，然后对其进行随机化操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:44.935503Z",
     "iopub.status.busy": "2020-09-22T22:26:44.934795Z",
     "iopub.status.idle": "2020-09-22T22:26:44.937252Z",
     "shell.execute_reply": "2020-09-22T22:26:44.936758Z"
    },
    "id": "6jAeYkTIi9-2"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:44.945141Z",
     "iopub.status.busy": "2020-09-22T22:26:44.944239Z",
     "iopub.status.idle": "2020-09-22T22:26:44.948104Z",
     "shell.execute_reply": "2020-09-22T22:26:44.947516Z"
    },
    "id": "Qd544E-Sh63L"
   },
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "  \n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4JEHrJXeG5k"
   },
   "source": [
    "你可以使用 `tf.data.Dataset.take` 与 `print` 来查看 `(example, label)` 对的外观。`numpy` 属性显示每个 Tensor 的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:44.952570Z",
     "iopub.status.busy": "2020-09-22T22:26:44.951923Z",
     "iopub.status.idle": "2020-09-22T22:26:46.242708Z",
     "shell.execute_reply": "2020-09-22T22:26:46.242047Z"
    },
    "id": "gywKlN0xh6u5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'To Ida; in his presence once arrived,'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Such now appears th' o'er-ruling sov'reign will\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Them so prepared the King of men beheld'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'mourn you, but the eddies of Scamander shall bear you into the broad'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'there was no life left in him.'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n"
     ]
    }
   ],
   "source": [
    "for ex in all_labeled_data.take(5):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rrpU2_sfDh0"
   },
   "source": [
    "## 将文本编码成数字\n",
    "\n",
    "机器学习基于的是数字而非文本，所以字符串需要被转化成数字列表。\n",
    "为了达到此目的，我们需要构建文本与整数的一一映射。\n",
    "\n",
    "### 建立词汇表\n",
    "\n",
    "\n",
    "首先，通过将文本标记为单独的单词集合来构建词汇表。在 TensorFlow 和 Python 中均有很多方法来达成这一目的。在本教程中:\n",
    "\n",
    "1. 迭代每个样本的 `numpy` 值。\n",
    "2. 使用 `tfds.features.text.Tokenizer` 来将其分割成 `token`。\n",
    "3. 将这些 `token` 放入一个 Python 集合中，借此来清除重复项。\n",
    "4. 获取该词汇表的大小以便于以后使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:46.248303Z",
     "iopub.status.busy": "2020-09-22T22:26:46.247639Z",
     "iopub.status.idle": "2020-09-22T22:26:52.739984Z",
     "shell.execute_reply": "2020-09-22T22:26:52.740398Z"
    },
    "id": "YkHtbGnDh6mg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17178"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W35VJqAh9zs"
   },
   "source": [
    "### 样本编码\n",
    "\n",
    "通过传递 `vocabulary_set` 到 `tfds.features.text.TokenTextEncoder` 来构建一个编码器。编码器的 `encode` 方法传入一行文本，返回一个整数列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:52.768534Z",
     "iopub.status.busy": "2020-09-22T22:26:52.767862Z",
     "iopub.status.idle": "2020-09-22T22:26:52.770215Z",
     "shell.execute_reply": "2020-09-22T22:26:52.769694Z"
    },
    "id": "gkxJIVAth6j0"
   },
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6S5Qyabi-vo"
   },
   "source": [
    "你可以尝试运行这一行代码并查看输出的样式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:52.774587Z",
     "iopub.status.busy": "2020-09-22T22:26:52.773928Z",
     "iopub.status.idle": "2020-09-22T22:26:54.049147Z",
     "shell.execute_reply": "2020-09-22T22:26:54.048624Z"
    },
    "id": "jgxPZaxUuTbk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'To Ida; in his presence once arrived,'\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:54.053737Z",
     "iopub.status.busy": "2020-09-22T22:26:54.052864Z",
     "iopub.status.idle": "2020-09-22T22:26:54.055538Z",
     "shell.execute_reply": "2020-09-22T22:26:54.056059Z"
    },
    "id": "XoVpKR3qj5yb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15746, 11433, 8394, 9006, 379, 3463, 17072]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9qHM0v8k_Mg"
   },
   "source": [
    "现在，在数据集上运行编码器（通过将编码器打包到 `tf.py_function` 并且传参至数据集的 `map` 方法的方式来运行）。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:54.063939Z",
     "iopub.status.busy": "2020-09-22T22:26:54.062692Z",
     "iopub.status.idle": "2020-09-22T22:26:54.115544Z",
     "shell.execute_reply": "2020-09-22T22:26:54.114962Z"
    },
    "id": "HcIQ7LOTh6eT"
   },
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "  encoded_text = encoder.encode(text_tensor.numpy())\n",
    "  return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "  # py_func doesn't set the shape of the returned tensors.\n",
    "  encoded_text, label = tf.py_function(encode, \n",
    "                                       inp=[text, label], \n",
    "                                       Tout=(tf.int64, tf.int64))\n",
    "\n",
    "  # `tf.data.Datasets` work best if all components have a shape set\n",
    "  #  so set the shapes manually: \n",
    "  encoded_text.set_shape([None])\n",
    "  label.set_shape([])\n",
    "\n",
    "  return encoded_text, label\n",
    "\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YZToSXSm0qr"
   },
   "source": [
    "## 将数据集分割为测试集和训练集且进行分支\n",
    "\n",
    "使用 `tf.data.Dataset.take` 和 `tf.data.Dataset.skip` 来建立一个小一些的测试数据集和稍大一些的训练数据集。\n",
    "\n",
    "在数据集被传入模型之前，数据集需要被分批。最典型的是，每个分支中的样本大小与格式需要一致。但是数据集中样本并不全是相同大小的（每行文本字数并不相同）。因此，使用 `tf.data.Dataset.padded_batch`（而不是 `batch` ）将样本填充到相同的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:54.121496Z",
     "iopub.status.busy": "2020-09-22T22:26:54.120447Z",
     "iopub.status.idle": "2020-09-22T22:26:54.127466Z",
     "shell.execute_reply": "2020-09-22T22:26:54.127901Z"
    },
    "id": "r-rmbijQh6bf"
   },
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE)\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xdz7SVwmqi1l"
   },
   "source": [
    "现在，test_data 和 train_data 不是（ `example, label` ）对的集合，而是批次的集合。每个批次都是一对（*多样本*, *多标签* ），表示为数组。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:54.133243Z",
     "iopub.status.busy": "2020-09-22T22:26:54.132236Z",
     "iopub.status.idle": "2020-09-22T22:26:55.724185Z",
     "shell.execute_reply": "2020-09-22T22:26:55.724731Z"
    },
    "id": "kMslWfuwoqpB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(16,), dtype=int64, numpy=\n",
       " array([15746, 11433,  8394,  9006,   379,  3463, 17072,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0])>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "sample_text[0], sample_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI4I6_Sa0vWu"
   },
   "source": [
    "由于我们引入了一个新的 token 来编码（填充零），因此词汇表大小增加了一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:55.729121Z",
     "iopub.status.busy": "2020-09-22T22:26:55.728494Z",
     "iopub.status.idle": "2020-09-22T22:26:55.730779Z",
     "shell.execute_reply": "2020-09-22T22:26:55.730173Z"
    },
    "id": "IlD1Lli91vuc"
   },
   "outputs": [],
   "source": [
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8SUhGFNsmRi"
   },
   "source": [
    "## 建立模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:55.734870Z",
     "iopub.status.busy": "2020-09-22T22:26:55.734232Z",
     "iopub.status.idle": "2020-09-22T22:26:55.744847Z",
     "shell.execute_reply": "2020-09-22T22:26:55.744226Z"
    },
    "id": "QJgI1pow2YR9"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi0iiKLTKdoF"
   },
   "source": [
    "第一层将整数表示转换为密集矢量嵌入。更多内容请查阅 [Word Embeddings](../../tutorials/sequences/word_embeddings) 教程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:55.754373Z",
     "iopub.status.busy": "2020-09-22T22:26:55.752847Z",
     "iopub.status.idle": "2020-09-22T22:26:55.766995Z",
     "shell.execute_reply": "2020-09-22T22:26:55.766428Z"
    },
    "id": "DR6-ctbY638P"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Embedding(vocab_size, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8OJOPohKh1q"
   },
   "source": [
    "下一层是 [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 层，它允许模型利用上下文中理解单词含义。 LSTM 上的双向包装器有助于模型理解当前数据点与其之前和之后的数据点的关系。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:55.794413Z",
     "iopub.status.busy": "2020-09-22T22:26:55.793538Z",
     "iopub.status.idle": "2020-09-22T22:26:56.207021Z",
     "shell.execute_reply": "2020-09-22T22:26:56.206315Z"
    },
    "id": "x6rnq6DN_WUs"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdffbMr5LF1g"
   },
   "source": [
    "最后，我们将获得一个或多个紧密连接的层，其中最后一层是输出层。输出层输出样本属于各个标签的概率，最后具有最高概率的分类标签即为最终预测结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:56.214197Z",
     "iopub.status.busy": "2020-09-22T22:26:56.213409Z",
     "iopub.status.idle": "2020-09-22T22:26:56.236363Z",
     "shell.execute_reply": "2020-09-22T22:26:56.235750Z"
    },
    "id": "QTEaNSnLCsv5"
   },
   "outputs": [],
   "source": [
    "# 一个或多个紧密连接的层\n",
    "# 编辑 `for` 行的列表去检测层的大小\n",
    "for units in [64, 64]:\n",
    "  model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "\n",
    "# 输出层。第一个参数是标签个数。\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLHPU8q5DLi_"
   },
   "source": [
    "最后，编译这个模型。对于一个 softmax 分类模型来说，通常使用 `sparse_categorical_crossentropy` 作为其损失函数。你可以尝试其他的优化器，但是 `adam` 是最常用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:56.250830Z",
     "iopub.status.busy": "2020-09-22T22:26:56.250011Z",
     "iopub.status.idle": "2020-09-22T22:26:56.257847Z",
     "shell.execute_reply": "2020-09-22T22:26:56.257323Z"
    },
    "id": "pkTBUVO4h6Y5"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM-HLo5NDhql"
   },
   "source": [
    "## 训练模型\n",
    "\n",
    "利用提供的数据训练出的模型有着不错的精度（大约 83% ）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:26:56.262625Z",
     "iopub.status.busy": "2020-09-22T22:26:56.261937Z",
     "iopub.status.idle": "2020-09-22T22:28:06.124759Z",
     "shell.execute_reply": "2020-09-22T22:28:06.124229Z"
    },
    "id": "aLtO33tNh6V8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "697/697 [==============================] - 10s 14ms/step - loss: 0.5181 - accuracy: 0.7457 - val_loss: 0.3855 - val_accuracy: 0.8222\n",
      "Epoch 2/3\n",
      "697/697 [==============================] - 9s 13ms/step - loss: 0.2985 - accuracy: 0.8685 - val_loss: 0.3635 - val_accuracy: 0.8350\n",
      "Epoch 3/3\n",
      "697/697 [==============================] - 9s 13ms/step - loss: 0.2242 - accuracy: 0.9027 - val_loss: 0.3794 - val_accuracy: 0.8246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4ff462aba8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=3, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T22:28:06.129627Z",
     "iopub.status.busy": "2020-09-22T22:28:06.128981Z",
     "iopub.status.idle": "2020-09-22T22:28:08.881952Z",
     "shell.execute_reply": "2020-09-22T22:28:08.881428Z"
    },
    "id": "KTPCYf_Jh6TH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 18ms/step - loss: 0.3794 - accuracy: 0.8246\n",
      "\n",
      "Eval loss: 0.3794495761394501, Eval accuracy: 0.8245999813079834\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "\n",
    "print('\\nEval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
